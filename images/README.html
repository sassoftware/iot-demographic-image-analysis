<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>

<link href='https://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color: #ffffff; --text-color: #333333; --select-text-bg-color: #B5D6FC; --select-text-font-color: auto; --monospace: "Lucida Console",Consolas,"Courier",monospace; --title-bar-height: 20px; }
.mac-os-11 { --title-bar-height: 28px; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
h1, h2, h3, h4, h5 { white-space: pre-wrap; }
body { margin: 0px; padding: 0px; height: auto; inset: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 36px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
.typora-export .task-list-item input { pointer-events: none; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; image-orientation: from-image; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px; }
thead, tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-fences-adv-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
svg { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li blockquote { margin: 1rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; border-color: transparent !important; padding-top: 0px !important; padding-bottom: 0px !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  .typora-export #write { break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  .is-mac table { break-inside: avoid; }
  #write > p:nth-child(1) { margin-top: 0px; }
  .typora-export-show-outline .typora-export-sidebar { display: none; }
  figure { overflow-x: visible; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
.MathJax_ref { fill: currentcolor; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.reversefootnote { font-family: ui-monospace, sans-serif; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.6; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
.md-expand mark .md-meta { opacity: 0.3 !important; }
mark .md-meta { color: rgb(0, 0, 0); }
@media print {
  .typora-export h1, .typora-export h2, .typora-export h3, .typora-export h4, .typora-export h5, .typora-export h6 { break-inside: avoid; }
}
.md-diagram-panel .messageText { stroke: none !important; }
.md-diagram-panel .start-state { fill: var(--node-fill); }
.md-diagram-panel .edgeLabel rect { opacity: 1 !important; }
.md-fences.md-fences-math { font-size: 1em; }
.md-fences-advanced:not(.md-focus) { padding: 0px; white-space: nowrap; border: 0px; }
.md-fences-advanced:not(.md-focus) { background: inherit; }
.typora-export-show-outline .typora-export-content { max-width: 1440px; margin: auto; display: flex; flex-direction: row; }
.typora-export-sidebar { width: 300px; font-size: 0.8rem; margin-top: 80px; margin-right: 18px; }
.typora-export-show-outline #write { --webkit-flex: 2; flex: 2 1 0%; }
.typora-export-sidebar .outline-content { position: fixed; top: 0px; max-height: 100%; overflow: hidden auto; padding-bottom: 30px; padding-top: 60px; width: 300px; }
@media screen and (max-width: 1024px) {
  .typora-export-sidebar, .typora-export-sidebar .outline-content { width: 240px; }
}
@media screen and (max-width: 800px) {
  .typora-export-sidebar { display: none; }
}
.outline-content li, .outline-content ul { margin-left: 0px; margin-right: 0px; padding-left: 0px; padding-right: 0px; list-style: none; overflow-wrap: anywhere; }
.outline-content ul { margin-top: 0px; margin-bottom: 0px; }
.outline-content strong { font-weight: 400; }
.outline-expander { width: 1rem; height: 1.42857rem; position: relative; display: table-cell; vertical-align: middle; cursor: pointer; padding-left: 4px; }
.outline-expander::before { content: ""; position: relative; font-family: Ionicons; display: inline-block; font-size: 8px; vertical-align: middle; }
.outline-item { padding-top: 3px; padding-bottom: 3px; cursor: pointer; }
.outline-expander:hover::before { content: ""; }
.outline-h1 > .outline-item { padding-left: 0px; }
.outline-h2 > .outline-item { padding-left: 1em; }
.outline-h3 > .outline-item { padding-left: 2em; }
.outline-h4 > .outline-item { padding-left: 3em; }
.outline-h5 > .outline-item { padding-left: 4em; }
.outline-h6 > .outline-item { padding-left: 5em; }
.outline-label { cursor: pointer; display: table-cell; vertical-align: middle; text-decoration: none; color: inherit; }
.outline-label:hover { text-decoration: underline; }
.outline-item:hover { border-color: rgb(245, 245, 245); background-color: var(--item-hover-bg-color); }
.outline-item:hover { margin-left: -28px; margin-right: -28px; border-left: 28px solid transparent; border-right: 28px solid transparent; }
.outline-item-single .outline-expander::before, .outline-item-single .outline-expander:hover::before { display: none; }
.outline-item-open > .outline-item > .outline-expander::before { content: ""; }
.outline-children { display: none; }
.info-panel-tab-wrapper { display: none; }
.outline-item-open > .outline-children { display: block; }
.typora-export .outline-item { padding-top: 1px; padding-bottom: 1px; }
.typora-export .outline-item:hover { margin-right: -8px; border-right: 8px solid transparent; }
.typora-export .outline-expander::before { content: "+"; font-family: inherit; top: -1px; }
.typora-export .outline-expander:hover::before, .typora-export .outline-item-open > .outline-item > .outline-expander::before { content: "−"; }
.typora-export-collapse-outline .outline-children { display: none; }
.typora-export-collapse-outline .outline-item-open > .outline-children, .typora-export-no-collapse-outline .outline-children { display: block; }
.typora-export-no-collapse-outline .outline-expander::before { content: "" !important; }
.typora-export-show-outline .outline-item-active > .outline-item .outline-label { font-weight: 700; }
.md-inline-math-container mjx-container { zoom: 0.95; }
mjx-container { break-inside: avoid; }
.md-alert.md-alert-note { border-left-color: rgb(9, 105, 218); }
.md-alert.md-alert-important { border-left-color: rgb(130, 80, 223); }
.md-alert.md-alert-warning { border-left-color: rgb(154, 103, 0); }
.md-alert.md-alert-tip { border-left-color: rgb(31, 136, 61); }
.md-alert.md-alert-caution { border-left-color: rgb(207, 34, 46); }
.md-alert { padding: 0px 1em; margin-bottom: 16px; color: inherit; border-left: 0.25em solid rgb(0, 0, 0); }
.md-alert-text-note { color: rgb(9, 105, 218); }
.md-alert-text-important { color: rgb(130, 80, 223); }
.md-alert-text-warning { color: rgb(154, 103, 0); }
.md-alert-text-tip { color: rgb(31, 136, 61); }
.md-alert-text-caution { color: rgb(207, 34, 46); }
.md-alert-text { font-size: 0.9rem; font-weight: 700; }
.md-alert-text svg { fill: currentcolor; position: relative; top: 0.125em; margin-right: 1ch; overflow: visible; }
.md-alert-text-container::after { content: attr(data-text); text-transform: capitalize; pointer-events: none; margin-right: 1ch; }


.CodeMirror { height: auto; }
.CodeMirror.cm-s-inner { background: inherit; }
.CodeMirror-scroll { overflow: auto hidden; z-index: 3; }
.CodeMirror-gutter-filler, .CodeMirror-scrollbar-filler { background-color: rgb(255, 255, 255); }
.CodeMirror-gutters { border-right: 1px solid rgb(221, 221, 221); background: inherit; white-space: nowrap; }
.CodeMirror-linenumber { padding: 0px 3px 0px 5px; text-align: right; color: rgb(153, 153, 153); }
.cm-s-inner .cm-keyword { color: rgb(119, 0, 136); }
.cm-s-inner .cm-atom, .cm-s-inner.cm-atom { color: rgb(34, 17, 153); }
.cm-s-inner .cm-number { color: rgb(17, 102, 68); }
.cm-s-inner .cm-def { color: rgb(0, 0, 255); }
.cm-s-inner .cm-variable { color: rgb(0, 0, 0); }
.cm-s-inner .cm-variable-2 { color: rgb(0, 85, 170); }
.cm-s-inner .cm-variable-3 { color: rgb(0, 136, 85); }
.cm-s-inner .cm-string { color: rgb(170, 17, 17); }
.cm-s-inner .cm-property { color: rgb(0, 0, 0); }
.cm-s-inner .cm-operator { color: rgb(152, 26, 26); }
.cm-s-inner .cm-comment, .cm-s-inner.cm-comment { color: rgb(170, 85, 0); }
.cm-s-inner .cm-string-2 { color: rgb(255, 85, 0); }
.cm-s-inner .cm-meta { color: rgb(85, 85, 85); }
.cm-s-inner .cm-qualifier { color: rgb(85, 85, 85); }
.cm-s-inner .cm-builtin { color: rgb(51, 0, 170); }
.cm-s-inner .cm-bracket { color: rgb(153, 153, 119); }
.cm-s-inner .cm-tag { color: rgb(17, 119, 0); }
.cm-s-inner .cm-attribute { color: rgb(0, 0, 204); }
.cm-s-inner .cm-header, .cm-s-inner.cm-header { color: rgb(0, 0, 255); }
.cm-s-inner .cm-quote, .cm-s-inner.cm-quote { color: rgb(0, 153, 0); }
.cm-s-inner .cm-hr, .cm-s-inner.cm-hr { color: rgb(153, 153, 153); }
.cm-s-inner .cm-link, .cm-s-inner.cm-link { color: rgb(0, 0, 204); }
.cm-negative { color: rgb(221, 68, 68); }
.cm-positive { color: rgb(34, 153, 34); }
.cm-header, .cm-strong { font-weight: 700; }
.cm-del { text-decoration: line-through; }
.cm-em { font-style: italic; }
.cm-link { text-decoration: underline; }
.cm-error { color: red; }
.cm-invalidchar { color: red; }
.cm-constant { color: rgb(38, 139, 210); }
.cm-defined { color: rgb(181, 137, 0); }
div.CodeMirror span.CodeMirror-matchingbracket { color: rgb(0, 255, 0); }
div.CodeMirror span.CodeMirror-nonmatchingbracket { color: rgb(255, 34, 34); }
.cm-s-inner .CodeMirror-activeline-background { background: inherit; }
.CodeMirror { position: relative; overflow: hidden; }
.CodeMirror-scroll { height: 100%; outline: 0px; position: relative; box-sizing: content-box; background: inherit; }
.CodeMirror-sizer { position: relative; }
.CodeMirror-gutter-filler, .CodeMirror-hscrollbar, .CodeMirror-scrollbar-filler, .CodeMirror-vscrollbar { position: absolute; z-index: 6; display: none; outline: 0px; }
.CodeMirror-vscrollbar { right: 0px; top: 0px; overflow: hidden; }
.CodeMirror-hscrollbar { bottom: 0px; left: 0px; overflow: auto hidden; }
.CodeMirror-scrollbar-filler { right: 0px; bottom: 0px; }
.CodeMirror-gutter-filler { left: 0px; bottom: 0px; }
.CodeMirror-gutters { position: absolute; left: 0px; top: 0px; padding-bottom: 10px; z-index: 3; overflow-y: hidden; }
.CodeMirror-gutter { white-space: normal; height: 100%; box-sizing: content-box; padding-bottom: 30px; margin-bottom: -32px; display: inline-block; }
.CodeMirror-gutter-wrapper { position: absolute; z-index: 4; background: 0px 0px !important; border: none !important; }
.CodeMirror-gutter-background { position: absolute; top: 0px; bottom: 0px; z-index: 4; }
.CodeMirror-gutter-elt { position: absolute; cursor: default; z-index: 4; }
.CodeMirror-lines { cursor: text; }
.CodeMirror pre { border-radius: 0px; border-width: 0px; background: 0px 0px; font-family: inherit; font-size: inherit; margin: 0px; white-space: pre; overflow-wrap: normal; color: inherit; z-index: 2; position: relative; overflow: visible; }
.CodeMirror-wrap pre { overflow-wrap: break-word; white-space: pre-wrap; word-break: normal; }
.CodeMirror-code pre { border-right: 30px solid transparent; width: fit-content; }
.CodeMirror-wrap .CodeMirror-code pre { border-right: none; width: auto; }
.CodeMirror-linebackground { position: absolute; inset: 0px; z-index: 0; }
.CodeMirror-linewidget { position: relative; z-index: 2; overflow: auto; }
.CodeMirror-wrap .CodeMirror-scroll { overflow-x: hidden; }
.CodeMirror-measure { position: absolute; width: 100%; height: 0px; overflow: hidden; visibility: hidden; }
.CodeMirror-measure pre { position: static; }
.CodeMirror div.CodeMirror-cursor { position: absolute; visibility: hidden; border-right: none; width: 0px; }
.CodeMirror div.CodeMirror-cursor { visibility: hidden; }
.CodeMirror-focused div.CodeMirror-cursor { visibility: inherit; }
.cm-searching { background: rgba(255, 255, 0, 0.4); }
span.cm-underlined { text-decoration: underline; }
span.cm-strikethrough { text-decoration: line-through; }
.cm-tw-syntaxerror { color: rgb(255, 255, 255); background-color: rgb(153, 0, 0); }
.cm-tw-deleted { text-decoration: line-through; }
.cm-tw-header5 { font-weight: 700; }
.cm-tw-listitem:first-child { padding-left: 10px; }
.cm-tw-box { border-style: solid; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-color: inherit; border-top-width: 0px !important; }
.cm-tw-underline { text-decoration: underline; }
@media print {
  .CodeMirror div.CodeMirror-cursor { visibility: hidden; }
}


:root {
    --side-bar-bg-color: #fafafa;
    --control-text-color: #777;
}

@include-when-export url(https://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext);

/* open-sans-regular - latin-ext_latin */
  /* open-sans-italic - latin-ext_latin */
    /* open-sans-700 - latin-ext_latin */
    /* open-sans-700italic - latin-ext_latin */
  html {
    font-size: 16px;
    -webkit-font-smoothing: antialiased;
}

body {
    font-family: "Open Sans","Clear Sans", "Helvetica Neue", Helvetica, Arial, 'Segoe UI Emoji', sans-serif;
    color: rgb(51, 51, 51);
    line-height: 1.6;
}

#write {
    max-width: 860px;
  	margin: 0 auto;
  	padding: 30px;
    padding-bottom: 100px;
}

@media only screen and (min-width: 1400px) {
	#write {
		max-width: 1024px;
	}
}

@media only screen and (min-width: 1800px) {
	#write {
		max-width: 1200px;
	}
}

#write > ul:first-child,
#write > ol:first-child{
    margin-top: 30px;
}

a {
    color: #4183C4;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}
h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}
h1 tt,
h1 code {
    font-size: inherit;
}
h2 tt,
h2 code {
    font-size: inherit;
}
h3 tt,
h3 code {
    font-size: inherit;
}
h4 tt,
h4 code {
    font-size: inherit;
}
h5 tt,
h5 code {
    font-size: inherit;
}
h6 tt,
h6 code {
    font-size: inherit;
}
h1 {
    font-size: 2.25em;
    line-height: 1.2;
    border-bottom: 1px solid #eee;
}
h2 {
    font-size: 1.75em;
    line-height: 1.225;
    border-bottom: 1px solid #eee;
}

/*@media print {
    .typora-export h1,
    .typora-export h2 {
        border-bottom: none;
        padding-bottom: initial;
    }

    .typora-export h1::after,
    .typora-export h2::after {
        content: "";
        display: block;
        height: 100px;
        margin-top: -96px;
        border-top: 1px solid #eee;
    }
}*/

h3 {
    font-size: 1.5em;
    line-height: 1.43;
}
h4 {
    font-size: 1.25em;
}
h5 {
    font-size: 1em;
}
h6 {
   font-size: 1em;
    color: #777;
}
p,
blockquote,
ul,
ol,
dl,
table{
    margin: 0.8em 0;
}
li>ol,
li>ul {
    margin: 0 0;
}
hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

li p.first {
    display: inline-block;
}
ul,
ol {
    padding-left: 30px;
}
ul:first-child,
ol:first-child {
    margin-top: 0;
}
ul:last-child,
ol:last-child {
    margin-bottom: 0;
}
blockquote {
    border-left: 4px solid #dfe2e5;
    padding: 0 15px;
    color: #777777;
}
blockquote blockquote {
    padding-right: 0;
}
table {
    padding: 0;
    word-break: initial;
}
table tr {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}
table tr:nth-child(2n),
thead {
    background-color: #f8f8f8;
}
table th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    margin: 0;
    padding: 6px 13px;
}
table td {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 6px 13px;
}
table th:first-child,
table td:first-child {
    margin-top: 0;
}
table th:last-child,
table td:last-child {
    margin-bottom: 0;
}

.CodeMirror-lines {
    padding-left: 4px;
}

.code-tooltip {
    box-shadow: 0 1px 1px 0 rgba(0,28,36,.3);
    border-top: 1px solid #eef2f2;
}

.md-fences,
code,
tt {
    border: 1px solid #e7eaed;
    background-color: #f8f8f8;
    border-radius: 3px;
    padding: 0;
    padding: 2px 4px 0px 4px;
    font-size: 0.9em;
}

code {
    background-color: #f3f4f4;
    padding: 0 2px 0 2px;
}

.md-fences {
    margin-bottom: 15px;
    margin-top: 15px;
    padding-top: 8px;
    padding-bottom: 6px;
}


.md-task-list-item > input {
  margin-left: -1.3em;
}

@media print {
    html {
        font-size: 13px;
    }
    pre {
        page-break-inside: avoid;
        word-wrap: break-word;
    }
}

.md-fences {
	background-color: #f8f8f8;
}
#write pre.md-meta-block {
	padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
	bottom: .375rem;
}

.md-mathjax-midline {
    background: #fafafa;
}

#write>h3.md-focus:before{
	left: -1.5625rem;
	top: .375rem;
}
#write>h4.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h5.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h6.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: #a7a7a7;
    opacity: 1;
}

.md-toc { 
    margin-top:20px;
    padding-bottom:20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

/** focus mode */
.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header, .context-menu, .megamenu-content, footer{
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state{
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.mac-os #write{
    caret-color: AccentColor;
}

.md-lang {
    color: #b4654d;
}

/*.html-for-mac {
    --item-hover-bg-color: #E6F0FE;
}*/

#md-notification .btn {
    border: 0;
}

.dropdown-menu .divider {
    border-color: #e5e5e5;
    opacity: 0.4;
}

.ty-preferences .window-content {
    background-color: #fafafa;
}

.ty-preferences .nav-group-item.active {
    color: white;
    background: #999;
}

.menu-item-container a.menu-style-btn {
    background-color: #f5f8fa;
    background-image: linear-gradient( 180deg , hsla(0, 0%, 100%, 0.8), hsla(0, 0%, 100%, 0)); 
}



</style><title>README</title>
</head>
<body class='typora-export os-windows'><div class='typora-export-content'>
<div id='write'  class=''><h1 id='demographic-image-analysis'><span>Demographic Image Analysis</span></h1><p><img src="https://img.shields.io/badge/Category-Computer Vision-blue"><span> </span><img src="https://img.shields.io/badge/Sub--Category-Object Detection + Image Detection-yellowgreen"><span> </span><img src="https://img.shields.io/badge/Difficulty-Advanced-yellow"><span> </span><img src="https://img.shields.io/badge/Analytical%20Method-YOLO + CNN-brightgreen"></p><p><span>In this repository, you&#39;ll see how to connect to an RTSP stream of image data is transformed or enhanced in real-time to deliver demographic data.</span></p><h2 id='overview'><span>Overview</span></h2><p><span>This repository will show you how to use two of the products found in SAS Analytics for IoT to train and build a demographic detector which may be used in a retail setting.  </span></p><ul><li><p><a href='https://www.sas.com/en_us/software/visual-data-mining-machine-learning.html'><span>SAS Visual Data Mining and Machine Learning</span></a></p></li><li><p><a href='https://www.sas.com/en_us/software/event-stream-processing.html'><span>SAS Event Stream Processing</span></a></p></li></ul><p><span>These two products provide the tools needed when training, testing and running computer vision models.  This repository will cover the following topics: </span></p><ul><li><p><span>Define the use case for our retail example</span></p></li><li><p><span>Gathering and displaying demographic data, using object detection and image recognition </span></p></li><li><p><span>Learn the basics of computer vision</span></p></li><li><p><span>Learn how to train CV models using the SAS Deep Learning Python (DLPY) package</span></p></li><li><p><span>How to deploy the trained CV models using SAS ESP </span></p></li><li><p><span>How to present the demographic data effectively using a Grafana dashboard</span></p></li></ul><h2 id='use-case'><span>Use Case</span></h2><p><span>As the proprietor of a retail establishment, envision gaining a better understanding of your customers. Imagine gaining insights into the emotional engagement of customers as they explore your store, knowing the demographic preferences for specific store aisles, distinguishing between male and female customer attractions. Wouldn&#39;t that be invaluable?  Using computer vision and streaming analytics it is possible to do this in real-time.  All that&#39;s required is a camera connected through RTSP, complemented by an object detection model pinpointing human faces, alongside image recognition models discerning additional details. The outcomes are gathered and channeled into a dashboard, enabling real-time analysis.</span></p><h2 id='gathering-and-displaying-demographic-data-a-retail-example'><span>Gathering and displaying demographic data: a retail example</span></h2><p><span>Using two products that are embedded in the SAS Analytics for IoT solution, </span><a href='https://www.sas.com/en_us/software/visual-data-mining-machine-learning.html'><span>SAS Visual Data Mining and Machine Learning</span></a><span> (VDMML) and </span><a href='https://www.sas.com/en_us/software/event-stream-processing.html'><span>SAS Event Stream Processing</span></a><span> (ESP),  let&#39;s explain how to train and deploy various demographic analytic models to create a single solution that connects directly to an RTSP connected security camera. Through this connection, processing incoming images using pre-training analytical models produce demographic statistics. </span></p><p><span>The following models will need to be trained for this use case:</span></p><ul><li><p><span>Face Detection - Process incoming images and detect human faces in the video stream</span></p></li><li><p><span>Age Classification - Determine approximate age grouped by category</span></p></li><li><p><span>Gender Classification - Determine male or female</span></p></li><li><p><span>Emotion Classification - Group by these classes:  Happy, Neutral, Sad, Fear, Surprise, Angry</span></p></li></ul><p><span>The example Grafana dashboard looks as follows:</span></p><p><img src="./images/image-20240130141003218.png" referrerpolicy="no-referrer" alt="image-20240130141003218"></p><p><span>There are various graphs displaying demographic statistics for the last 100 observations as well as a live video stream which shows the current face that has been detected via the RTSP stream.  The image has been augmented to show the current results from the scoring algorithms. </span></p><h2 id='first-off-lets-talk-computer-vision'><span>First off, let&#39;s talk computer vision</span></h2><p><span>Computer vision (CV) techniques provide the ability to acquire, process and analyze incoming images.  This analysis produces numerical results in the form of predictions based on the classes we define.  In this example, we need to create four separate CV models.  First, we need an object detection model which will not only give us the probability there is a human face in the incoming image, but it will also give us the coordinates of that face in the image, in the form of a bounding box.  Using the box coordinates we can then crop just the face from the incoming image and send that data to the next analytical model.  Consider this example:</span></p><p><img src="./images/yolo.png" referrerpolicy="no-referrer"></p><p><span>Here I’m using a YOLO (You Only Look Once) </span><a href='https://blogs.sas.com/content/subconsciousmusings/2019/03/21/building-a-yolo-object-detection-model-using-sas/'><span>object detection model</span></a><span> to find all the human faces in the incoming video stream.  In the case of multiple faces being detected, multiple cropped faces are passed to the downstream models as separate events.  </span></p><h2 id='next-we-train-our-models'><span>Next, we train our models</span></h2><p><span>Before I can build an application that uses analytical models to predict outcomes, we need to train them.  The training process for CV involves classifying images and separating these images into datasets that can then be fed into a </span><a href='https://www.sas.com/en_us/insights/analytics/machine-learning.html'><span>machine learning</span></a><span> model such as ResNet50, VGG16, Darknet etc.  This stage of the process is completed using the </span><a href='https://github.com/sassoftware/python-dlpy'><span>SAS Deep Learning Python (DLPy)</span></a><span> package which provides the high-level </span><strong><span>Python APIs</span></strong><span> to the deep learning methods in </span><strong><span>SAS Visual Data Mining and Machine Learning</span></strong><span> (VDMML).</span></p><p><img src="./images/processFlow.png" referrerpolicy="no-referrer"><span>	</span></p><p><span>As the previous diagram illustrates, four separate datasets were created to support model training.  Because I needed to tweak each dataset to get the best possible output, each image dataset supports the training of one model.  Face detection was trained using a Yolo V2 architecture while age, gender and emotions were trained using a ResNet50 architecture.  For example, when training the gender model, image data is loaded into SAS Viya and VDMML using DLPy.  Deep learning algorithms are then invoked as each image is processed to create a portable analytics file called an ASTORE file.  VDMML is GPU-enabled so that training times are greatly improved.  A typical training exercise contains these steps:</span></p><ul><li><p><span>Setup libraries and launch CAS</span></p></li><li><p><span>Load and explore the training data</span></p></li><li><p><span>Prepare the data for modeling</span></p></li><li><p><span>Specify the model architecture, configure model parameters and import pre-trained weights</span></p></li><li><p><span>Fit the image detection and classification model</span></p></li><li><p><span>Evaluate the newly created image classification model</span></p></li><li><p><span>Visualize model results</span></p></li><li><p><span>Save model as ASTORE for deployment</span></p></li></ul><h2 id='training'><span>Training</span></h2><p><span>Model training is optional  since the pre-trained .astore files have been provided.  However, contained in the training directory of this repository are the Jupyter notebooks which were used during the demographic model training. Please refer to these notebooks in order to learn how to train your own computer vision analytic models.  Please refer to the following for details: </span></p><ul><li><p><a href='./training/Age.ipynb'><span>Age training example</span></a></p></li><li><p><a href='./training/Emotions.ipynb'><span>Emotions training example</span></a></p></li><li><p><a href='./training/Gender.ipynb'><span>Gender training example</span></a></p></li></ul><p><span>It is recommended that no less than 1000 images be used for each class you intend to train and more images are always better.  For example, each age group or emotion class should contain no less than 1000 images.  These images are also subdivided into two datasets.  The first is for training and the second is a testing dataset.  Therefore, an additional 200 images for testing is also recommended.  Image size should be no less than 224 by 224 pixels.  Image databases such as </span><a href='https://storage.googleapis.com/openimages/web/index.html'><span>Open Image</span></a><span> are a great place to start.      </span></p><p><span>For more information about the DLPy high-level Python APIs which allows you to build deep learning models please see the following examples:</span></p><ul><li><p><a href='https://github.com/sassoftware/python-dlpy/tree/master/examples/image_classification'><span>Image Classification</span></a></p></li><li><p><a href='https://github.com/sassoftware/python-dlpy/tree/master/examples/object_detection'><span>Object Detection</span></a></p></li></ul><h2 id='now-use-streaming-analytics'><span>Now use streaming analytics</span></h2><p><span>Streaming analytics is defined as the ability to constantly calculate statistical analytics on an incoming stream of data.  In our case, that stream of data is the images coming from the camera.  </span><strong><span>SAS Event Stream Processing (ESP)</span></strong><span>, which is part of the </span><strong><span>SAS Analytics for IoT</span></strong><span> solution, provides the ability to deploy our newly trained analytical models, in the form of ASTORE files, at the edge.  With ESP you can ingest, filter and transform your data in stream.  I like to think of it as enabling business rules to gain extra business value.  For example, let&#39;s say your company wanted to track how many happy women between the ages of 10 and 30 walked by the frozen food section on Tuesday when you were running a sale, verses a normal day when the sale is not running.   </span><strong><span>ESP</span></strong><span> gives you the capability to make that happen.  There is a saying here at SAS, &quot;Without deployment, analytics is only a science experiment.&quot;</span></p><p><span>This diagram illustrates an overview of this project deployment architecture.</span></p><p><img src="./images/image-20240130145551889.png" referrerpolicy="no-referrer" alt="image-20240130145551889"></p><p><span>Here we can see the flow of information through the system and highlight some key points:</span></p><ul><li><p><strong><span>ESP is built for speed.</span></strong><span> Although, there are many methods of ingesting data into ESP (REST, MQTT, MQ), to make this superfast I used a RTSP connector which allows me to directly connect ESP to the incoming video stream from the camera.  I also took advantage of ESP&#39;s multithreaded capability by scoring age, gender and emotion simultaneously each in its own thread. </span></p></li><li><p><strong><span>ESP integrates with open source.</span></strong><span> You can easily call python scripts in stream from the ESP model flow.  This allows further integration with other open source packages such as OpenCV.  Using Python and OpenCV, the images were cropped, resized, reshaped and colors were manipulated.  Anything is possible. </span></p></li><li><p><strong><span>Retention is amazing.</span></strong><span> Retention defines a group of events over a time or event count.  Instead of doing analytics on each image individually, you can now take a group of images and create new event data – like the number of men over the last hour or total number of kids today.  It is very powerful.</span></p></li><li><p><strong><span>ESP</span></strong><span> includes a powerful graphical development platform. Using </span><strong><span>ESP Studio</span></strong><span> models such as these may be created without any coding.  For example, ingesting images via the RTSP connector is as easy as dragging a window onto the canvas and filling out an RTSP specifications form. </span></p></li><li><p><strong><span>ESP Grafana plugin</span></strong><span> seamlessly combines Grafana&#39;s intuitive dashboarding with ESP&#39;s versatility, offering real-time insights and dynamic visualization.</span></p></li></ul><p>&nbsp;</p><p>&nbsp;</p><h2 id='esp-project-detail'><span>ESP Project Detail</span></h2><p><span>The ESP project is best described in phases.  </span></p><ul><li><p><span>Project package contents </span></p></li><li><p><span>Data ingestion </span></p></li><li><p><span>Analytical scoring </span></p></li><li><p><span>OpenCV integration using python</span></p></li><li><p><span>Grafana plugin integration</span></p></li></ul><p><img src="./images/image-20240131110603268.png" referrerpolicy="no-referrer" alt="image-20240131110603268"><span>	</span></p><p><span>As you can see the model is very readable and intuitive.  Data flows in from the top blue box which represents our connection to the video stream. From there the image is resized and passed to our first algorithm which detects human faces.  Using the ESP python window openCV is used to crop faces directly from the ESP server without using MAS or external python libraries.  The cropped face image is then passed to our three demographic computer vision models.  Model results are used to annotate the image in real time as well as calculate totals for the last 100 events coming into the project.  These totals are then sent to Grafana and our dashboard is created. </span></p><h3 id='project-package'><span>Project Package</span></h3><p><span>Building your own demographic detector involves creating an ESP project package which includes the following trained models.</span></p><ul><li><p><span>Age.astore - CNN model which detects age </span></p></li><li><p><span>Emotion.astore - CNN model which detects emotions</span></p></li><li><p><span>Gender.astore -  CNN model which determines gender</span></p></li><li><p><span>faceDetection.Tiny-Yolo2.416.v1.astore - Object detection model which locates human faces in images</span></p></li></ul><p><span>A project package is useful for organizing related content within a single package.  Once downloaded please include them in your project package as follows. It is best practice to create a project package and place your analytical models in the analytics directory.   The model reader window will use this file location to load the model.  Once the location of the ASTORE is known the XML is very straight forward.  An ESP environment variable is used to define the location.  </span></p><p><img src="./images/image-20240131103800927.png" referrerpolicy="no-referrer" alt="Project Package"><span>	</span></p><h3 id='model-reader-window'><span>Model Reader Window</span></h3><p><span>As mentioned above the model reader window references the models stored in the project package standard locations using an ESP environment variable.   This allows for better organization and standardization.  For the age astore model the reader window would be defined as follows: </span></p><pre class="md-fences md-end-block ty-contain-cm modeLoaded" spellcheck="false" lang="xml"><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang="xml"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9.52344px; left: 8px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 0px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation" style=""><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: 0px; width: 0px;"></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-tag cm-bracket">&lt;</span><span class="cm-tag">window-model-reader</span> <span class="cm-attribute">name</span>=<span class="cm-string">"readagemodel"</span> <span class="cm-attribute">pubsub</span>=<span class="cm-string">"true"</span> <span class="cm-attribute">model-type</span>=<span class="cm-string">"astore"</span><span class="cm-tag cm-bracket">&gt;</span></span></pre></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp;<span class="cm-tag cm-bracket">&lt;</span><span class="cm-tag">description</span><span class="cm-tag cm-bracket">&gt;</span><span class="cm-atom">&lt;![CDATA[Read model which determines age]]&gt;</span><span class="cm-tag cm-bracket">&lt;/</span><span class="cm-tag">description</span><span class="cm-tag cm-bracket">&gt;</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp;<span class="cm-tag cm-bracket">&lt;</span><span class="cm-tag">parameters</span><span class="cm-tag cm-bracket">&gt;</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp;<span class="cm-tag cm-bracket">&lt;</span><span class="cm-tag">properties</span><span class="cm-tag cm-bracket">&gt;</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp;<span class="cm-tag cm-bracket">&lt;</span><span class="cm-tag">property</span> <span class="cm-attribute">name</span>=<span class="cm-string">"reference"</span><span class="cm-tag cm-bracket">&gt;</span><span class="cm-atom">&lt;![CDATA[@ESP_PROJECT_HOME@/analytics/Age.astore]]&gt;</span><span class="cm-tag cm-bracket">&lt;/</span><span class="cm-tag">property</span><span class="cm-tag cm-bracket">&gt;</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp;<span class="cm-tag cm-bracket">&lt;</span><span class="cm-tag">property</span> <span class="cm-attribute">name</span>=<span class="cm-string">"usegpuesp"</span><span class="cm-tag cm-bracket">&gt;</span><span class="cm-atom">&lt;![CDATA[1]]&gt;</span><span class="cm-tag cm-bracket">&lt;/</span><span class="cm-tag">property</span><span class="cm-tag cm-bracket">&gt;</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp;<span class="cm-tag cm-bracket">&lt;</span><span class="cm-tag">property</span> <span class="cm-attribute">name</span>=<span class="cm-string">"NDEVICES"</span><span class="cm-tag cm-bracket">&gt;</span><span class="cm-atom">&lt;![CDATA[1]]&gt;</span><span class="cm-tag cm-bracket">&lt;/</span><span class="cm-tag">property</span><span class="cm-tag cm-bracket">&gt;</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp;<span class="cm-tag cm-bracket">&lt;</span><span class="cm-tag">property</span> <span class="cm-attribute">name</span>=<span class="cm-string">"DEVICE0"</span><span class="cm-tag cm-bracket">&gt;</span><span class="cm-atom">&lt;![CDATA[0]]&gt;</span><span class="cm-tag cm-bracket">&lt;/</span><span class="cm-tag">property</span><span class="cm-tag cm-bracket">&gt;</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp;<span class="cm-tag cm-bracket">&lt;/</span><span class="cm-tag">properties</span><span class="cm-tag cm-bracket">&gt;</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp;<span class="cm-tag cm-bracket">&lt;/</span><span class="cm-tag">parameters</span><span class="cm-tag cm-bracket">&gt;</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-tag cm-bracket">&lt;/</span><span class="cm-tag">window-model-reader</span><span class="cm-tag cm-bracket">&gt;</span></span></pre></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 277px;"></div><div class="CodeMirror-gutters" style="display: none; height: 277px;"></div></div></div></pre><p><span>This is also where you specify GPU usage for this model.  If usegpuesp is set to 1, ESP will use the gpu when scoring this ASTORE.</span><span>		</span></p><h3 id='esp-video-capture-connector'><span>ESP Video Capture Connector</span></h3><p><span>The </span><strong><span>SAS ESP video capture connector</span></strong><span> is a tool that reads an input video stream and publishes each extracted image frame into a Source window as a unique event that contains a binary blob field.  It is one of the many connectors and adapters that extend SAS functionality.</span></p><p><span>Using ESP Studio configure a Source window publisher as follows:</span></p><p><img src="./images/image-20240131095813525.png" referrerpolicy="no-referrer" alt="image-20240131095813525"><span>	</span></p><p><span>  </span></p><h3 id='analytical-scoring'><span>Analytical Scoring</span></h3><p><span>ESP is configured to call 4  ASTORE models.  The first streaming analytics model performs real-time object detection, where the object is defined as a human face.  The output is mapped to the following schema.  </span><em><span>nObjects</span></em><span> defines how many faces were found and the remaining schema entries define where in the image the object was located.  Sample of object detection schema: </span></p><p><img src="./images/image-20240131141710248.png" referrerpolicy="no-referrer" alt="image-20240131141710248"><span>	</span></p><p><span>These values will later be used to draw bounding boxes and create new cropped images of the face which was detected.   The face image is then passed to the image classification models to determine mood, gender and age of the person.  After all the models are run, we have 3 important strings in our schema that represent the results of our efforts. </span></p><p><img src="./images/image-20240131141847189.png" referrerpolicy="no-referrer" alt="image-20240131141847189"><span>	</span></p><p><span>The counts assigned to these labels are aggregated later in the project to generate totals, which are then visualized as graphs on the Grafana dashboard.</span></p><p><span>	</span></p><h3 id='opencv-using-the-esp-python-window'><span>OpenCV Using the ESP Python Window</span></h3><p><span>ESP now contains a Python window that can be used to call python directly.  Python window empowers you to craft functions within Python programs, generating SAS Event Stream Processing events.  The Python environment includes most commonly used packages including Numpy and OpenCV.   This project leverages these packages to manipulate images in the following ways:</span></p><ul><li><p><span>Cropping</span></p></li><li><p><span>Annotating </span></p></li><li><p><span>Reshaping </span></p></li><li><p><span>Converting to greyscale</span></p></li><li><p><span>Drawing bounding boxes</span></p></li></ul><p><span>Images are passed to the Python window as BLOBs.  No B64 encoding is necessary.  This improves performance.  A handy Python code editor is also included to make things easy. </span></p><p><img src="images/image-20240131150343398.png" referrerpolicy="no-referrer" alt="image-20240131150343398"><span>	</span></p><p><span>As with the Lua window a function called create is called and passed the input schema in a variable called data.  A function that uses openCV call crops_draw_bboxes() is called to do just that. Crop faces out of the main image creating smaller images that can be passed to the demographic ASTOREs for scoring.  </span></p><h3 id='grafana-plugin-setup'><span>Grafana Plugin Setup</span></h3><p><span>The new </span><a href='https://github.com/sassoftware/grafana-esp-plugin/blob/main/README.md'><span>SAS ESP grafana plugin</span></a><span> is a data source plugin that allows you to stream data from SAS Event Stream Processing (ESP) servers in a Kubernetes environment to Grafana dashboards With this plugin, you can visualize and monitor real-time streaming data from ESP projects and windows, and apply filters and aggregations to customize your data views. The plugin supports both ESP Pub/Sub and REST APIs, and provides a user-friendly interface to discover and select ESP servers, projects, and windows.  The plugin also enables you to use Grafana alerting and annotation features on ESP data.</span></p><p><span>First the SAS ESP plugin must be used to create a connection to the SAS ESP Studio which will be running the project.  It looks like this:</span></p><p><img src="./images/image-20231204142514126.png" referrerpolicy="no-referrer" alt="image-20240201142001911"><span>	</span></p><p><span>Simply enter the url of the ESP Studio which is running the project and save and test your connection.  When the connection is made create a Dashboard and start adding visualizations.   The visualization edit panel will include a section called data source.  Pick the data source which was created above and then select the ESP window and fields which are to be used by this visualization.</span></p><p><span> The visualizations used above are: </span></p><ol start='' ><li><p><span>Base64 Image/Video/Audio/PDF</span></p></li><li><p><span>Bar Gauge</span></p></li><li><p><span>Gauge</span></p></li><li><p><span>Pie Chart</span></p></li></ol><p><span>Here is an example of how you might configure the SAS Event Stream Processing data source for the bar gauge.  </span></p><p><img src="./images/image-20240131152353308.png" referrerpolicy="no-referrer" alt="image-20240131152353308"><span>	</span></p><p><span>Variables from the output schema of the ESP window which you are connecting to are selected.  These variables are then used to Grafana to product the graph.  </span></p><p>&nbsp;</p><h2 id='conclusion'><span>Conclusion</span></h2><p><strong><span>As you can see, SAS Analytics for IoT</span></strong><span> provides you with all the tools you’ll need to quickly go from concept to production.  Although this example consumed image data, you can use any type of data.  This comprehensive solution also provides tools to maintain and govern model development, as well as everything you need to visualize and cleanse new data sources.   Let’s see where your imagination takes you!  I’d love to hear how you put SAS Analytics for IoT to work for your company.</span></p><h2 id='prerequisites'><span>Prerequisites</span></h2><p><span>List of required software offered as part of </span><a href='https://www.sas.com/en_us/software/analytics-iot.html'><span>SAS Analytics for IoT</span></a></p><p><strong><span>Training</span></strong><span>   </span></p><ul><li><p><a href='https://www.sas.com/en_us/software/visual-data-mining-machine-learning.html'><span>SAS Visual Data Mining and Machine Learning</span></a></p></li><li><p><a href='https://sassoftware.github.io/python-swat/'><span>SAS Scripting Wrapper for Analytics Transfer</span></a></p></li><li><p><a href='https://sassoftware.github.io/python-dlpy/'><span>SAS Deep Learning Python Interface</span></a></p></li></ul><p><strong><span>Streaming</span></strong><span>   </span></p><ul><li><p><a href='https://www.sas.com/en_us/software/event-stream-processing.html'><span>SAS Event Stream Processing</span></a></p></li><li><p><span>A RTSP stream of image data </span></p></li></ul><h2 id='installation'><span>Installation</span></h2><p><span>This accelerator assumes you have access to an Analytics for IoT installation.  This includes SAS Event Stream Processing and VDMML.  </span></p><h2 id='contributing'><span>Contributing</span></h2><p><span>This repository is not open for external contributions.</span></p><h2 id='license'><span>License</span></h2><p><span>This project is licensed under the </span><a href='LICENSE'><span>Apache 2.0 License</span></a><span>.</span></p><h2 id='additional-resources'><span>Additional Resources</span></h2><p><span>Additional resources might include the following:</span></p><ul><li><p><a href='https://www.sas.com/en_us/software/analytics-iot.html'><span>SAS Analytics for IoT</span></a></p></li><li><p><a href='https://www.sas.com/en_us/software/visual-data-mining-machine-learning.html'><span>SAS Visual Data Mining and Machine Learning</span></a></p></li><li><p><a href='https://www.sas.com/en_us/software/event-stream-processing.html'><span>SAS Event Stream Processing</span></a></p></li><li><p><a href='https://developer.sas.com/home.html'><span>SAS for Developers</span></a></p></li><li><p><a href='https://sassoftware.github.io/python-dlpy/'><span>SAS Deep Learning Python Interface</span></a></p></li><li><p><a href='https://sassoftware.github.io/python-swat/'><span>SAS Scripting Wrapper for Analytics Transfer</span></a></p></li><li><p><a href='https://communities.sas.com/'><span>SAS Communities</span></a></p></li></ul><p>&nbsp;</p></div></div>
</body>
</html>